{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'imdb_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m negative_words \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mbad\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mterrible\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mawful\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhorrible\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Load and preprocess your dataset (replace 'data' with your dataset)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mimdb_dataset.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m text: text\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39msplit())  \u001b[39m# Tokenization and lowercase\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Calculate sentiment scores for each document\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'imdb_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained GloVe model\n",
    "glove_model = {}\n",
    "glove_path = '../../IR/glove.6B.50d.txt'  # Adjust the path to your GloVe model\n",
    "\n",
    "\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        vector = np.array(parts[1:], dtype=np.float32)\n",
    "        glove_model[word] = vector\n",
    "\n",
    "# Define your polarity words\n",
    "positive_words = [\"good\", \"excellent\", \"great\", \"amazing\"]\n",
    "negative_words = [\"bad\", \"terrible\", \"awful\", \"horrible\"]\n",
    "\n",
    "# Load and preprocess your dataset (replace 'data' with your dataset)\n",
    "data = pd.read_csv(\"imdb_dataset.csv\")\n",
    "data['review'] = data['review'].apply(lambda text: text.lower().split())  # Tokenization and lowercase\n",
    "\n",
    "# Calculate sentiment scores for each document\n",
    "sentiments = []\n",
    "\n",
    "for text in data['review']:\n",
    "    positive_similarity = np.mean([np.dot(glove_model.get(word, np.zeros_like(glove_model[\"a\"])), glove_model.get(w, np.zeros_like(glove_model[\"a\"]))).tolist() for word in text for w in positive_words if word in glove_model])\n",
    "    negative_similarity = np.mean([np.dot(glove_model.get(word, np.zeros_like(glove_model[\"a\"])), glove_model.get(w, np.zeros_like(glove_model[\"a\"]))).tolist() for word in text for w in negative_words if word in glove_model])\n",
    "\n",
    "    if positive_similarity > negative_similarity:\n",
    "        sentiments.append(\"positive\")\n",
    "    elif negative_similarity > positive_similarity:\n",
    "        sentiments.append(\"negative\")\n",
    "    else:\n",
    "        sentiments.append(\"neutral\")\n",
    "\n",
    "# Add the sentiments to your dataset\n",
    "data['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/omkar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m sentiments \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m data[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     positive_similarity \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([np\u001b[39m.\u001b[39mdot(glove_model\u001b[39m.\u001b[39mget(word, np\u001b[39m.\u001b[39mzeros_like(glove_model[\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m])), glove_model\u001b[39m.\u001b[39mget(w, np\u001b[39m.\u001b[39mzeros_like(glove_model[\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m])))\u001b[39m.\u001b[39mtolist() \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m text \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m positive_words \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m glove_model])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W1sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     negative_similarity \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([np\u001b[39m.\u001b[39mdot(glove_model\u001b[39m.\u001b[39mget(word, np\u001b[39m.\u001b[39mzeros_like(glove_model[\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m])), glove_model\u001b[39m.\u001b[39mget(w, np\u001b[39m.\u001b[39mzeros_like(glove_model[\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m])))\u001b[39m.\u001b[39mtolist() \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m text \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m negative_words \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m glove_model])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mif\u001b[39;00m positive_similarity \u001b[39m>\u001b[39m negative_similarity:\n",
      "\u001b[1;32m/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m sentiments \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m data[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     positive_similarity \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([np\u001b[39m.\u001b[39mdot(glove_model\u001b[39m.\u001b[39mget(word, np\u001b[39m.\u001b[39;49mzeros_like(glove_model[\u001b[39m\"\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m\"\u001b[39;49m])), glove_model\u001b[39m.\u001b[39mget(w, np\u001b[39m.\u001b[39mzeros_like(glove_model[\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m])))\u001b[39m.\u001b[39mtolist() \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m text \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m positive_words \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m glove_model])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W1sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     negative_similarity \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([np\u001b[39m.\u001b[39mdot(glove_model\u001b[39m.\u001b[39mget(word, np\u001b[39m.\u001b[39mzeros_like(glove_model[\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m])), glove_model\u001b[39m.\u001b[39mget(w, np\u001b[39m.\u001b[39mzeros_like(glove_model[\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m])))\u001b[39m.\u001b[39mtolist() \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m text \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m negative_words \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m glove_model])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omkar/LP4/MLDS/Assignment6/Polarity.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mif\u001b[39;00m positive_similarity \u001b[39m>\u001b[39m negative_similarity:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mzeros_like\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/numpy/core/numeric.py:138\u001b[0m, in \u001b[0;36mzeros_like\u001b[0;34m(a, dtype, order, subok, shape)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_zeros_like_dispatcher)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mzeros_like\u001b[39m(a, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mK\u001b[39m\u001b[39m'\u001b[39m, subok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     78\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m    Return an array of zeros with the same shape and type as a given array.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     res \u001b[39m=\u001b[39m empty_like(a, dtype\u001b[39m=\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder, subok\u001b[39m=\u001b[39;49msubok, shape\u001b[39m=\u001b[39;49mshape)\n\u001b[1;32m    139\u001b[0m     \u001b[39m# needed instead of a 0 to get same result as zeros for string dtypes\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     z \u001b[39m=\u001b[39m zeros(\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mempty_like\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load the pre-trained GloVe model\n",
    "glove_model = {}\n",
    "glove_path = '../../IR/glove.6B.50d.txt'   # Adjust the path to your GloVe model\n",
    "\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        vector = np.array(parts[1:], dtype=np.float32)\n",
    "        glove_model[word] = vector\n",
    "\n",
    "# Define your polarity words\n",
    "positive_words = [\"good\", \"excellent\", \"great\", \"amazing\"]\n",
    "negative_words = [\"bad\", \"terrible\", \"awful\", \"horrible\"]\n",
    "\n",
    "# Load and preprocess your dataset (replace 'data' with your dataset)\n",
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "# Define a function for text preprocessing, including stop word removal\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stop words\n",
    "    return words\n",
    "\n",
    "data['review'] = data['review'].apply(preprocess_text)\n",
    "\n",
    "# Calculate sentiment scores for each document\n",
    "sentiments = []\n",
    "\n",
    "for text in data['review']:\n",
    "    positive_similarity = np.mean([np.dot(glove_model.get(word, np.zeros_like(glove_model[\"a\"])), glove_model.get(w, np.zeros_like(glove_model[\"a\"]))).tolist() for word in text for w in positive_words if word in glove_model])\n",
    "    negative_similarity = np.mean([np.dot(glove_model.get(word, np.zeros_like(glove_model[\"a\"])), glove_model.get(w, np.zeros_like(glove_model[\"a\"]))).tolist() for word in text for w in negative_words if word in glove_model])\n",
    "\n",
    "    if positive_similarity > negative_similarity:\n",
    "        sentiments.append(\"positive\")\n",
    "    elif negative_similarity > positive_similarity:\n",
    "        sentiments.append(\"negative\")\n",
    "    else:\n",
    "        sentiments.append(\"neutral\")\n",
    "\n",
    "# Add the sentiments to your dataset\n",
    "data['sentiment'] = sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/omkar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = {}\n",
    "glove_path = '../../IR/glove.6B.50d.txt'\n",
    "\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        vector = np.array(parts[1:], dtype=np.float32)\n",
    "        glove_model[word] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = [\"good\", \"excellent\", \"great\", \"amazing\"]\n",
    "negative_words = [\"bad\", \"terrible\", \"awful\", \"horrible\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags using regular expressions\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stop words\n",
    "    return words\n",
    "\n",
    "data['review'] = data['review'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, reviewers, mentioned, watching, 1, oz, e...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[wonderful, little, production., &lt;br, /&gt;&lt;br, /...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[basically, there's, family, little, boy, (jak...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[petter, mattei's, \"love, time, money\", visual...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [one, reviewers, mentioned, watching, 1, oz, e...  positive\n",
       "1  [wonderful, little, production., <br, /><br, /...  positive\n",
       "2  [thought, wonderful, way, spend, time, hot, su...  positive\n",
       "3  [basically, there's, family, little, boy, (jak...  negative\n",
       "4  [petter, mattei's, \"love, time, money\", visual...  positive"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "\n",
    "for text in data['review']:\n",
    "    positive_similarity = []\n",
    "    negative_similarity = []\n",
    "\n",
    "    for word in text:\n",
    "        if word in glove_model:\n",
    "            positive_similarity.extend([np.dot(glove_model[word], glove_model[p]).tolist() for p in positive_words if p in glove_model])\n",
    "            negative_similarity.extend([np.dot(glove_model[word], glove_model[n]).tolist() for n in negative_words if n in glove_model])\n",
    "\n",
    "    if positive_similarity and negative_similarity:\n",
    "        avg_positive_similarity = np.mean(positive_similarity)\n",
    "        avg_negative_similarity = np.mean(negative_similarity)\n",
    "\n",
    "        if avg_positive_similarity > avg_negative_similarity:\n",
    "            sentiments.append(\"positive\")\n",
    "        elif avg_negative_similarity > avg_positive_similarity:\n",
    "            sentiments.append(\"negative\")\n",
    "        else:\n",
    "            sentiments.append(\"neutral\")\n",
    "    else:\n",
    "        sentiments.append(\"neutral\")\n",
    "\n",
    "# Add the sentiments to your dataset\n",
    "data['sentiment2'] = sentiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    4973\n",
       "negative      27\n",
       "Name: sentiment2, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    2532\n",
       "positive    2468\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/omkar/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8075\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.76      0.90      0.82       199\n",
      "         pos       0.88      0.72      0.79       201\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.82      0.81      0.81       400\n",
      "weighted avg       0.82      0.81      0.81       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Download the movie reviews dataset\n",
    "nltk.download(\"movie_reviews\")\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "docs = [(list(movie_reviews.words(fileid)), category)\n",
    "        for category in movie_reviews.categories()\n",
    "        for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Create a list of documents and labels\n",
    "documents = [' '.join(doc) for doc, category in docs]\n",
    "labels = [category for doc, category in docs]\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Bag of Words (BoW) representation of the text data\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Convert the BoW representation to TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Transform and classify the test data\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred, target_names=movie_reviews.categories())\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (2964351460.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[27], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    2)Recognize optical character using ANN\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
